{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aede3ea",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c283b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import naive_bayes\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from preprocessing.preprocessor import Preprocessor\n",
    "from datasets.fake_news_detection import FakeNewsDetectionDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc96211",
   "metadata": {},
   "source": [
    "## Inspect test and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e0b068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Labels                                               News\n",
      "0       1  A little less than a decade ago, hockey fans w...\n",
      "1       1  The writers of the HBO series The Sopranos too...\n",
      "2       1  Despite claims from the TV news outlet to offe...\n",
      "3       1  After receiving 'subpar' service and experienc...\n",
      "4       1  After watching his beloved Seattle Mariners pr...\n",
      "   Labels                                               News\n",
      "0       1  When so many actors seem content to churn out ...\n",
      "1       1   In what football insiders are calling an unex...\n",
      "2       1  In a freak accident following Game 3 of the N....\n",
      "3       1  North Koreas official news agency announced to...\n",
      "4       1  The former Alaska Governor Sarah Palin would b...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../raw_data/fulltrain.csv', header=None)\n",
    "test = pd.read_csv('../raw_data/balancedtest.csv', header=None)\n",
    "train.columns=['Labels', 'News']\n",
    "test.columns=['Labels', 'News']\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56e77c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    17870\n",
      "1    14047\n",
      "4     9995\n",
      "2     6942\n",
      "Name: Labels, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Distribution of train labels'}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train['Labels'].value_counts())\n",
    "train['Labels'].value_counts().plot(kind = 'bar', title = 'Distribution of train labels')\n",
    "##Imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dd112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    750\n",
      "2    750\n",
      "3    750\n",
      "4    750\n",
      "Name: Labels, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Distribution of test labels'}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test['Labels'].value_counts())\n",
    "test['Labels'].value_counts().plot(kind = 'bar', title = 'Distribution of test labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d3fa0",
   "metadata": {},
   "source": [
    "### Outdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808be671",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##For some reason couldnt import the preprocessor from the other folder. Idk if it's a Jupiter notebook thing\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "stopwords = list(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_text(s, remove_stopwords=True, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer, stemmer=None, lemmatizer=None, lowercase=True):\n",
    "    # Throw error if both stemmer and lemmatizer are not None\n",
    "    if stemmer is not None and lemmatizer is not None:\n",
    "        raise ValueError(\"Stemmer and Lemmatizer cannot both be not None\")\n",
    "\n",
    "    # Tokenize with default tokenizer\n",
    "    token_list = tokenizer.tokenize(s)\n",
    "\n",
    "    # Fix contractions\n",
    "    if fix_contractions:\n",
    "        new_token_list = []\n",
    "        for token in token_list:\n",
    "            new_token = contractions.fix(token)\n",
    "            if (len(new_token.split()) == 2):\n",
    "                new_token_list.append(new_token.split()[0])\n",
    "                new_token_list.append(new_token.split()[1])\n",
    "            else:\n",
    "                new_token_list.append(new_token)\n",
    "        token_list = new_token_list\n",
    "\n",
    "    # Stem or lemmatize if not None\n",
    "    if stemmer is not None:\n",
    "        for idx, token in enumerate(token_list):\n",
    "            token_list[idx] = stemmer.stem(token)\n",
    "    elif lemmatizer is not None:\n",
    "        # Pos tagger to tag all tokens\n",
    "        pos_tag_list = pos_tag(token_list)\n",
    "        for idx, (token, tag) in enumerate(pos_tag_list):\n",
    "            tag_simple = tag[0].lower()\n",
    "            if tag_simple in ['n', 'v', 'j']:\n",
    "                word_type = tag_simple.replace('j', 'a')\n",
    "            else:\n",
    "                word_type = 'n'\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos=word_type)\n",
    "            token_list[idx] = lemmatized_token\n",
    "\n",
    "    # Convert all tokens to lowercase if True\n",
    "    if lowercase:\n",
    "        token_list = [token.lower() for token in token_list]\n",
    "\n",
    "    # Remove all stopwords if True\n",
    "    if remove_stopwords:\n",
    "        token_list = [token for token in token_list if not token in stopwords]\n",
    "\n",
    "    # Remove all punctuation marks if True\n",
    "    if remove_punctuation:\n",
    "        token_list = [''.join(c for c in s if c not in string.punctuation) for s in token_list]\n",
    "        token_list = [token for token in token_list if len(token) > 0]\n",
    "\n",
    "    #return token_list\n",
    "    return ' '.join(token_list)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_train_punc = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_punc = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_train_stop_punc = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=False, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_stop_punc = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=False, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_train_con_punc = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_con_punc = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_train_stop_con_punc = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_stop_con_punc = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_train_stop = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_stop = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_train_con = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_con = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "\n",
    "\n",
    "x_train_stop_con = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=True, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_stop_con = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=True, fix_contractions=True, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_train_none = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "x_test_none = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer))\n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "x_train_punc_con_lemma = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer, lemmatizer=wordnet_lemmatizer))\n",
    "x_train_punc_con_stem = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer, stemmer=porter_stemmer))\n",
    "x_train_no_lemma = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer, lemmatizer=wordnet_lemmatizer))\n",
    "x_train_no_stem = train['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer, stemmer=porter_stemmer))\n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d539ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_test_punc_con_lemma = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer, lemmatizer=wordnet_lemmatizer))\n",
    "x_test_punc_con_stem = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=True, remove_punctuation=True,\n",
    "                    tokenizer=whitespace_tokenizer, stemmer=porter_stemmer))\n",
    "x_test_no_lemma = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer, lemmatizer=wordnet_lemmatizer))\n",
    "x_test_no_stem = test['News'].apply(lambda x : preprocess_text(x, remove_stopwords=False, fix_contractions=False, remove_punctuation=False,\n",
    "                    tokenizer=whitespace_tokenizer, stemmer=porter_stemmer))\n",
    "                    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01066ba",
   "metadata": {},
   "source": [
    "## Use Preprocessor to Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a115f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control which linguistic preprocessing steps should run.\n",
    "preprocessor_none = Preprocessor(perform_case_folding=False,\n",
    "                            remove_stop_words=False,\n",
    "                            remove_punctuation=False,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_stop = Preprocessor(perform_case_folding=False,\n",
    "                            remove_stop_words=True,\n",
    "                            remove_punctuation=False,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_punc = Preprocessor(perform_case_folding=False,\n",
    "                            remove_stop_words=False,\n",
    "                            remove_punctuation=True,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_case = Preprocessor(perform_case_folding=True,\n",
    "                            remove_stop_words=False,\n",
    "                            remove_punctuation=False,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_stop_case = Preprocessor(perform_case_folding=True,\n",
    "                            remove_stop_words=True,\n",
    "                            remove_punctuation=False,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_stop_punc = Preprocessor(perform_case_folding=False,\n",
    "                            remove_stop_words=True,\n",
    "                            remove_punctuation=True,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_punc_case = Preprocessor(perform_case_folding=True,\n",
    "                            remove_stop_words=False,\n",
    "                            remove_punctuation=True,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_stop_punc_case = Preprocessor(perform_case_folding=True,\n",
    "                            remove_stop_words=True,\n",
    "                            remove_punctuation=True,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=False)\n",
    "preprocessor_case_stem = Preprocessor(perform_case_folding=True,\n",
    "                            remove_stop_words=False,\n",
    "                            remove_punctuation=False,\n",
    "                            perform_lemmatization=False,\n",
    "                            perform_stemming=True)\n",
    "preprocessor_case_lemma = Preprocessor(perform_case_folding=True,\n",
    "                            remove_stop_words=False,\n",
    "                            remove_punctuation=False,\n",
    "                            perform_lemmatization=True,\n",
    "                            perform_stemming=False)\n",
    "\n",
    "def join(list):\n",
    "    string = \"\"\n",
    "    for word in list:\n",
    "        string = string + word + \" \"\n",
    "    return string.rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fe426",
   "metadata": {},
   "source": [
    "## All permutations of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "174a9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_none = train['News'].apply(lambda x : join(preprocessor_none.process(x)))\n",
    "x_test_none = test['News'].apply(lambda x : join(preprocessor_none.process(x)))\n",
    "x_train_punc = train['News'].apply(lambda x : join(preprocessor_punc.process(x)))\n",
    "x_test_punc = test['News'].apply(lambda x : join(preprocessor_punc.process(x)))\n",
    "x_train_case = train['News'].apply(lambda x: join(preprocessor_case.process(x)))\n",
    "x_test_case = test['News'].apply(lambda x: join(preprocessor_case.process(x)))\n",
    "x_train_stop = train['News'].apply(lambda x: join(preprocessor_stop.process(x)))\n",
    "x_test_stop = test['News'].apply(lambda x: join(preprocessor_stop.process(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "574e2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stop_case = train['News'].apply(lambda x: join(preprocessor_stop_case.process(x)))\n",
    "x_test_stop_case = test['News'].apply(lambda x: join(preprocessor_stop_case.process(x)))\n",
    "x_train_stop_punc = train['News'].apply(lambda x: join(preprocessor_stop_punc.process(x)))\n",
    "x_test_stop_punc = test['News'].apply(lambda x: join(preprocessor_stop_punc.process(x)))\n",
    "x_train_punc_case = train['News'].apply(lambda x: join(preprocessor_punc_case.process(x)))\n",
    "x_test_punc_case = test['News'].apply(lambda x: join(preprocessor_punc_case.process(x)))\n",
    "x_train_stop_punc_case = train['News'].apply(lambda x: join(preprocessor_stop_punc_case.process(x)))\n",
    "x_test_stop_punc_case = test['News'].apply(lambda x: join(preprocessor_stop_punc_case.process(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "afc598b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_case_stem = train['News'].apply(lambda x : join(preprocessor_case_stem.process(x)))\n",
    "x_test_case_stem = test['News'].apply(lambda x : join(preprocessor_case_stem.process(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "aceff666",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_case_lemma = train['News'].apply(lambda x : join(preprocessor_case_lemma.process(x)))\n",
    "x_test_case_lemma = test['News'].apply(lambda x : join(preprocessor_case_lemma.process(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2136ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       When so many actors seem content to churn out ...\n",
      "1       In what football insiders are calling an unexp...\n",
      "2       In a freak accident following Game 3 of the N....\n",
      "3       North Koreas official news agency announced to...\n",
      "4       The former Alaska Governor Sarah Palin would b...\n",
      "                              ...                        \n",
      "2995    The Air Force mistakenly gave rival companies ...\n",
      "2996    The United Nations climate chief on Friday cha...\n",
      "2997    River Plate midfielder Diego Buonanotte has un...\n",
      "2998    Lawmakers were on the brink Tuesday of exempti...\n",
      "2999    The Pentagon which is processing bids on a new...\n",
      "Name: News, Length: 3000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_test_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1d4fc",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce36a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train['Labels'])\n",
    "test_y = encoder.fit_transform(test['Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9e7ccef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train_case\n",
    "X_test = x_test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b26c3996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        a little less than a decade ago , hockey fans ...\n",
      "1        the writers of the hbo series the sopranos too...\n",
      "2        despite claims from the tv news outlet to offe...\n",
      "3        after receiving 'subpar ' service and experien...\n",
      "4        after watching his beloved seattle mariners pr...\n",
      "                               ...                        \n",
      "48849    the ruling kuomintang ( kmt ) has claimed owne...\n",
      "48850    the taipei city government has encouraged the ...\n",
      "48851    president ma ying-jeou said friday that a park...\n",
      "48852    the families of the four people who were kille...\n",
      "48853    the ministry of finance will make public on sa...\n",
      "Name: News, Length: 48854, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80dce1",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0111eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', lowercase=False)\n",
    "count_vectorizer.fit(X_train)\n",
    "x_train_count = count_vectorizer.transform(X_train)\n",
    "x_test_count = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace83d8",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer - Word / Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0271e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_word = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000, lowercase=False)\n",
    "tfidf_vectorizer_word.fit(X_train)\n",
    "x_train_tfidf_word = tfidf_vectorizer_word.transform(X_train)\n",
    "x_test_tfidf_word = tfidf_vectorizer_word.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2bef8c",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b928d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=25000, lowercase=False)\n",
    "tfidf_vectorizer_bigram.fit(X_train)\n",
    "x_train_tfidf_bigram = tfidf_vectorizer_bigram.transform(X_train)\n",
    "x_test_tfidf_bigram = tfidf_vectorizer_bigram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f474b1",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer - Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ab8dbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_ngram2 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=25000, lowercase=False)\n",
    "tfidf_vectorizer_ngram2.fit(X_train)\n",
    "x_train_tfidf_ngram2 = tfidf_vectorizer_ngram2.transform(X_train)\n",
    "x_test_tfidf_ngram2 = tfidf_vectorizer_ngram2.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74827a17",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer - Unigrams and Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2cfdbb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13564/2207232872.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf_vectorizer_ngram3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'\\w{1,}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m27500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfidf_vectorizer_ngram3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_train_tfidf_ngram3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer_ngram3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_test_tfidf_ngram3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer_ngram3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0;32m   1219\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[0;32m   1222\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[1;34m(self, X, vocabulary)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0msorted_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m         \u001b[0mmap_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m             \u001b[0mmap_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "tfidf_vectorizer_ngram3 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=27500)\n",
    "tfidf_vectorizer_ngram3.fit(X_train)\n",
    "x_train_tfidf_ngram3 = tfidf_vectorizer_ngram3.transform(X_train)\n",
    "x_test_tfidf_ngram3 = tfidf_vectorizer_ngram3.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaaa55b",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer - Char Ngram(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b0d53e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viveg\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "##Not feasible -- Too little information\n",
    "tfidf_vectorizer_char = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=5000)\n",
    "tfidf_vectorizer_char.fit(X_train)\n",
    "x_train_tfidf_char = tfidf_vectorizer_char.transform(X_train)\n",
    "x_test_tfidf_char = tfidf_vectorizer_char.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb01d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score([2]*len(test_y), test_y, average='macro'))\n",
    "print(metrics.accuracy_score([2]*len(test_y), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bab312c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter:\n",
    "alpha_value=1\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def predict(model, X_valid):\n",
    "    y_pred = model.predict(X_valid)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7053bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer f1_score is 0.6665350822689343\n",
      "Count vectorizer accuracy is 0.6806666666666666\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_count_model = train_model(naive_bayes.MultinomialNB(alpha=alpha_value), x_train_count, train_y)\n",
    "y_pred_nb_count = predict(naive_bayes_count_model, x_test_count)\n",
    "nb_count_f1_score = metrics.f1_score(y_pred_nb_count, test_y, average='macro')\n",
    "nb_count_accuracy_score = metrics.accuracy_score(y_pred_nb_count, test_y)\n",
    "print(\"Count vectorizer f1_score is \" + str(nb_count_f1_score))\n",
    "print(\"Count vectorizer accuracy is \" + str(nb_count_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ab2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Count vectorizer f1 and accuracy scores:\n",
    "No preprocessing:\n",
    "Count vectorizer f1_score is 0.6648366966603887\n",
    "Count vectorizer accuracy is 0.679\n",
    "Contractions removed:\n",
    "Count vectorizer f1_score is 0.6695618110202328\n",
    "Count vectorizer accuracy is 0.6866666666666666\n",
    "Stopwords removed:\n",
    "Count vectorizer f1_score is 0.6677972020499174\n",
    "Count vectorizer accuracy is 0.6806666666666666\n",
    "Punctuation removed:\n",
    "Count vectorizer f1_score is 0.6264366613111475\n",
    "Count vectorizer accuracy is 0.6536666666666666\n",
    "Stopwords and contractions removed:\n",
    "Count vectorizer f1_score is 0.6669255857862734\n",
    "Count vectorizer accuracy is 0.6823333333333333\n",
    "Stopwords and punctuation removed:\n",
    "Count vectorizer f1_score is 0.6261753209216978\n",
    "Count vectorizer accuracy is 0.6506666666666666\n",
    "Contractions and punctuation removed:\n",
    "Count vectorizer f1_score is 0.6255990239756839\n",
    "Count vectorizer accuracy is 0.6533333333333333\n",
    "Stopwords, contractions and punctuation removed:\n",
    "Count vectorizer f1_score is 0.6288138324436103\n",
    "Count vectorizer accuracy is 0.6533333333333333\n",
    "Lemmatized:\n",
    "Count vectorizer f1_score is 0.6640627955785731\n",
    "Count vectorizer accuracy is 0.6776666666666666\n",
    "Lemmatized with contractions removed:\n",
    "Count vectorizer f1_score is 0.6690216115480041\n",
    "Count vectorizer accuracy is 0.6853333333333333\n",
    "Lemmatized with stopwords and contractions removed:\n",
    "Count vectorizer f1_score is 0.6645455317554574\n",
    "Count vectorizer accuracy is 0.6796666666666666\n",
    "Lemmatized with punctuation and contractions removed:\n",
    "Count vectorizer f1_score is 0.6209725947745535\n",
    "Count vectorizer accuracy is 0.6496666666666666\n",
    "Stemmed with contractions removed:\n",
    "Count vectorizer f1_score is 0.6560708632341219\n",
    "Count vectorizer accuracy is 0.674\n",
    "Stemmed with stopwords and contractions removed:\n",
    "Count vectorizer f1_score is 0.6523107919499966\n",
    "Count vectorizer accuracy is 0.6686666666666666\n",
    "Stemmed with punctuation and contractions removed:\n",
    "Count vectorizer f1_score is 0.61318342513182\n",
    "Count vectorizer accuracy is 0.643\n",
    "Stemmed:\n",
    "Count vectorizer f1_score is 0.6592195586487739\n",
    "Count vectorizer accuracy is 0.674\n",
    "'''\n",
    "'''\n",
    "New preprocessor:\n",
    "No preprocessing:\n",
    "Count vectorizer f1_score is 0.6605962257434558\n",
    "Count vectorizer accuracy is 0.676\n",
    "Punctuation removed:\n",
    "Count vectorizer f1_score is 0.6608669114101966\n",
    "Count vectorizer accuracy is 0.6763333333333333\n",
    "Case folding:\n",
    "Count vectorizer f1_score is 0.6652860068024109\n",
    "Count vectorizer accuracy is 0.6793333333333333\n",
    "Stopwords removed:\n",
    "Count vectorizer f1_score is 0.6505969425554999\n",
    "Count vectorizer accuracy is 0.6623333333333333\n",
    "Stopwords removed and case folding:\n",
    "Count vectorizer f1_score is 0.6618306648177975\n",
    "Count vectorizer accuracy is 0.6736666666666666\n",
    "Punctuation removed and case folding:\n",
    "Count vectorizer f1_score is 0.6658490233722193\n",
    "Count vectorizer accuracy is 0.68\n",
    "Stopwords and pucntuation removed:\n",
    "Count vectorizer f1_score is 0.6508342891131483\n",
    "Count vectorizer accuracy is 0.6626666666666666\n",
    "Stopwords removed and punctuation removed and case folding:\n",
    "Count vectorizer f1_score is 0.6623934412247338\n",
    "Count vectorizer accuracy is 0.6743333333333333\n",
    "Stemmed with case folding:\n",
    "Count vectorizer f1_score is 0.6627021965987446\n",
    "Count vectorizer accuracy is 0.675\n",
    "Lemmatized with case folding:\n",
    "Count vectorizer f1_score is 0.6665350822689343\n",
    "Count vectorizer accuracy is 0.6806666666666666\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4589bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf word f1_score is 0.6793754330291637\n",
      "Tfidf word accuracy is 0.683\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_tfidf_word_model = train_model(naive_bayes.MultinomialNB(alpha=alpha_value), x_train_tfidf_word, train_y)\n",
    "y_pred_nb_tfidf_word = predict(naive_bayes_tfidf_word_model, x_test_tfidf_word)\n",
    "nb_tfidf_word_f1_score = metrics.f1_score(y_pred_nb_tfidf_word, test_y, average='macro')\n",
    "nb_tfidf_word_accuracy_score = metrics.accuracy_score(y_pred_nb_tfidf_word, test_y)\n",
    "print(\"Tfidf word f1_score is \" + str(nb_tfidf_word_f1_score))\n",
    "print(\"Tfidf word accuracy is \" + str(nb_tfidf_word_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d3a8149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStopwords removed and punctuation removed:\\nTfidf word f1_score is3 0.6834935161682985\\nTfidf word accuracy is3 0.6853333333333333\\nStopwords removed:\\nTfidf word f1_score is3 0.6838634571798237\\nTfidf word accuracy is3 0.6856666666666666\\nNo preprocessing:\\nTfidf word f1_score is3 0.6845608624697852\\nTfidf word accuracy is3 0.688\\nLower case:\\nTfidf word f1_score is3 0.6845608624697852\\nTfidf word accuracy is3 0.688\\nPunctuation removed:\\nTfidf word f1_score is3 0.6845608624697852\\nTfidf word accuracy is3 0.688\\n\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Tfidf word f1 and accuracy scores:\n",
    "No preprocessing:\n",
    "Tfidf word f1_score is 0.683878861491291\n",
    "Tfidf word accuracy is 0.6873333333333334\n",
    "Contractions removed:\n",
    "Tfidf word f1_score is 0.6854522538517311\n",
    "Tfidf word accuracy is 0.6906666666666667\n",
    "Stopwords removed:\n",
    "Tfidf word f1_score is 0.689722124862688\n",
    "Tfidf word accuracy is 0.6923333333333334\n",
    "Punctuation removed:\n",
    "Tfidf word f1_score is 0.6776867759945222\n",
    "Tfidf word accuracy is 0.683\n",
    "Stopwords and contractions removed:\n",
    "Tfidf word f1_score is 0.6891682040511752\n",
    "Tfidf word accuracy is 0.693\n",
    "Stopwords and punctuation removed:\n",
    "Tfidf word f1_score is 0.6819728764781524\n",
    "Tfidf word accuracy is 0.6853333333333333\n",
    "Contractions and punctuation removed:\n",
    "Tfidf word f1_score is 0.6781793402029098\n",
    "Tfidf word accuracy is 0.683\n",
    "Stopwords, contractions and punctuation removed:\n",
    "Tfidf word f1_score is 0.6827353091503914\n",
    "Tfidf word accuracy is 0.686\n",
    "Lemmatized:\n",
    "Tfidf word f1_score is 0.6700652915974927\n",
    "Tfidf word accuracy is 0.6736666666666666\n",
    "Lemmatized with contractions removed:\n",
    "Tfidf word f1_score is 0.6759027511881394\n",
    "Tfidf word accuracy is 0.6813333333333333\n",
    "Lemmatized with contractions and stopwords removed:\n",
    "Tfidf word f1_score is 0.6784250758928283\n",
    "Tfidf word accuracy is 0.6823333333333333\n",
    "Lemmatized with punctuation and contractions removed:\n",
    "Tfidf word f1_score is 0.663616607958018\n",
    "Tfidf word accuracy is 0.6686666666666666\n",
    "Stemmed with contractions removed:\n",
    "Tfidf word f1_score is 0.6709296131945277\n",
    "Tfidf word accuracy is 0.6763333333333333\n",
    "Stemmed with contractions and stopwords removed:\n",
    "Tfidf word f1_score is 0.6725619904366508\n",
    "Tfidf word accuracy is 0.6766666666666666\n",
    "Stemmed with punctuation and contractions removed:\n",
    "Tfidf word f1_score is 0.6605212266562899\n",
    "Tfidf word accuracy is 0.6656666666666666\n",
    "Stemmed:\n",
    "Tfidf word f1_score is 0.6681187495585874\n",
    "Tfidf word accuracy is 0.672\n",
    "'''\n",
    "\n",
    "'''\n",
    "New preprocessor:\n",
    "No preprocessing:\n",
    "Tfidf word f1_score is 0.6863827846238891\n",
    "Tfidf word accuracy is 0.6893333333333334\n",
    "Case folding:\n",
    "Tfidf word f1_score is 0.6845608624697852\n",
    "Tfidf word accuracy is 0.688\n",
    "Stopword removal:\n",
    "Tfidf word f1_score is 0.6888903971314111\n",
    "Tfidf word accuracy is 0.6903333333333334\n",
    "Punctuation removal:\n",
    "Tfidf word f1_score is 0.6863827846238891\n",
    "Tfidf word accuracy is 0.6893333333333334\n",
    "Stopword removal with case folding:\n",
    "Tfidf word f1_score is 0.6838634571798237\n",
    "Tfidf word accuracy is 0.6856666666666666\n",
    "Punctuation removal with case folding:\n",
    "Tfidf word f1_score is 0.6845608624697852\n",
    "Tfidf word accuracy is 0.688\n",
    "Stopword and punctuation removal:\n",
    "Tfidf word f1_score is 0.6888903971314111\n",
    "Tfidf word accuracy is 0.6903333333333334\n",
    "Stopword and punctuation removal with case folding:\n",
    "Tfidf word f1_score is 0.6834935161682985\n",
    "Tfidf word accuracy is 0.6853333333333333\n",
    "Stemmed with case folding:\n",
    "Tfidf word f1_score is 0.6693093278147153\n",
    "Tfidf word accuracy is 0.673\n",
    "Lemmatized with case folding:\n",
    "Tfidf word f1_score is 0.6793754330291637\n",
    "Tfidf word accuracy is 0.683\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b08649f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf ngram2 f1_score is 0.69442532067018\n",
      "Tfidf ngram2 accuracy is 0.6983333333333334\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_tfidf_ngram2_model = train_model(naive_bayes.MultinomialNB(alpha=alpha_value), x_train_tfidf_ngram2, train_y)\n",
    "y_pred_nb_tfidf_ngram2 = predict(naive_bayes_tfidf_ngram2_model, x_test_tfidf_ngram2)\n",
    "nb_tfidf_ngram2_f1_score = metrics.f1_score(y_pred_nb_tfidf_ngram2, test_y, average='macro')\n",
    "nb_tfidf_ngram2_accuracy_score = metrics.accuracy_score(y_pred_nb_tfidf_ngram2, test_y)\n",
    "print(\"Tfidf ngram2 f1_score is \" + str(nb_tfidf_ngram2_f1_score))\n",
    "print(\"Tfidf ngram2 accuracy is \" + str(nb_tfidf_ngram2_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d89e0da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNew preprocessor:\\nNo preprocessing (25K):\\nTfidf ngram2 f1_score is 0.6861357773512551\\nTfidf ngram2 accuracy is 0.6903333333333334\\nCase folding:\\nTfidf ngram2 f1_score is 0.69442532067018\\nTfidf ngram2 accuracy is 0.6983333333333334\\nStopword removal:\\n\\n\\nStopwords removed and lowercase:\\nTfidf ngram2 f1_score is 0.6871153451437382\\nTfidf ngram2 accuracy is 0.6883333333333334\\nStopwords removed and punctuation removed and lowercase:\\nTfidf ngram2 f1_score is 0.6874877737745918\\nTfidf ngram2 accuracy is 0.6886666666666666\\nPunctuation removed and lowercase:\\nTfidf ngram2 f1_score is3 0.6927363814682612\\nTfidf ngram2 accuracy is3 0.6966666666666667\\n\\nLower case (22.5K):\\nTfidf ngram2 f1_score is3 0.6934778512658694\\nTfidf ngram2 accuracy is3 0.6973333333333334\\n25K:\\nTfidf ngram2 f1_score is 0.69442532067018\\nTfidf ngram2 accuracy is 0.6983333333333334\\n27.5K:\\nTfidf ngram2 f1_score is 0.6922133381924681\\nTfidf ngram2 accuracy is 0.6963333333333334\\n'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Tfidf ngram2 f1 and accuracy scores:\n",
    "No preprocessing(20K):\n",
    "Tfidf ngram2 f1_score is 0.6904927036593713\n",
    "Tfidf ngram2 accuracy is 0.695\n",
    "Contractions removed(20K):\n",
    "Tfidf ngram2 f1_score is 0.6957307273163922\n",
    "Tfidf ngram2 accuracy is 0.703\n",
    "Stopwords removed(20K):\n",
    "Tfidf ngram2 f1_score is 0.6878052273096324\n",
    "Tfidf ngram2 accuracy is 0.6893333333333334\n",
    "Punctuation removed(20K):\n",
    "Tfidf ngram2 f1_score is 0.6844410490110763\n",
    "Tfidf ngram2 accuracy is 0.6913333333333334\n",
    "Stopwords and contractions removed(20K):\n",
    "Tfidf ngram2 f1_score is 0.7010536400814287\n",
    "Tfidf ngram2 accuracy is 0.7036666666666667\n",
    "Stopwords and punctuation removed(20K):\n",
    "Tfidf ngram2 f1_score is 0.6891452524608814\n",
    "Tfidf ngram2 accuracy is 0.691\n",
    "Contractions and punctuation removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6827694473244238\n",
    "Tfidf ngram2 accuracy is 0.6896666666666667\n",
    "Stopwords, contractions and punctuation removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6916478434781799\n",
    "Tfidf ngram2 accuracy is 0.6936666666666667\n",
    "Lemmatized(20K):\n",
    "Tfidf ngram2 f1_score is 0.6905142045790961\n",
    "Tfidf ngram2 accuracy is 0.6943333333333334\n",
    "Lemmatized with contractions removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6951674648051098\n",
    "Tfidf ngram2 accuracy is 0.702\n",
    "Lemmatized with stopwords and contractions removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6958286539392421\n",
    "Tfidf ngram2 accuracy is 0.6986666666666667\n",
    "Lemmatized with punctuation and contractions removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6837571122200548\n",
    "Tfidf ngram2 accuracy is 0.6903333333333334\n",
    "Stemmed with contractions removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6885835360684373\n",
    "Tfidf ngram2 accuracy is 0.696\n",
    "Stemmed with contractions and stopwords removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6932509882462328\n",
    "Tfidf ngram2 accuracy is 0.697\n",
    "Stemmed with punctuation and contractions removed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6740400047721969\n",
    "Tfidf ngram2 accuracy is 0.6816666666666666\n",
    "Stemmed (20K):\n",
    "Tfidf ngram2 f1_score is 0.6810769148224473\n",
    "Tfidf ngram2 accuracy is 0.6863333333333334\n",
    "'''\n",
    "\n",
    "'''\n",
    "New preprocessor:\n",
    "No preprocessing (25K):\n",
    "Tfidf ngram2 f1_score is 0.6861357773512551\n",
    "Tfidf ngram2 accuracy is 0.6903333333333334\n",
    "Case folding:\n",
    "Tfidf ngram2 f1_score is 0.69442532067018\n",
    "Tfidf ngram2 accuracy is 0.6983333333333334\n",
    "Stopword removal:\n",
    "Tfidf ngram2 f1_score is 0.6823236331722874\n",
    "Tfidf ngram2 accuracy is 0.6833333333333333\n",
    "Punctuation removal:\n",
    "Tfidf ngram2 f1_score is 0.6858057709556158\n",
    "Tfidf ngram2 accuracy is 0.69\n",
    "Stopword removal with case folding:\n",
    "Tfidf ngram2 f1_score is 0.6856204471229342\n",
    "Tfidf ngram2 accuracy is 0.687\n",
    "Punctuation removal with case folding:\n",
    "Tfidf ngram2 f1_score is 0.6944109047073916\n",
    "Tfidf ngram2 accuracy is 0.6983333333333334\n",
    "Stopword and punctuation removal:\n",
    "Tfidf ngram2 f1_score is 0.6819949511926823\n",
    "Tfidf ngram2 accuracy is 0.683\n",
    "Stopword and punctuation removal with case folding:\n",
    "Tfidf ngram2 f1_score is 0.6862762490000844\n",
    "Tfidf ngram2 accuracy is 0.6876666666666666\n",
    "Stemmed with case folding:\n",
    "Tfidf ngram2 f1_score is 0.6819228180583291\n",
    "Tfidf ngram2 accuracy is 0.686\n",
    "Lemmatized with case folding:\n",
    "Tfidf ngram2 f1_score is 0.6909527289726216\n",
    "Tfidf ngram2 accuracy is 0.695\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Stopwords removed and lowercase:\n",
    "Tfidf ngram2 f1_score is 0.6871153451437382\n",
    "Tfidf ngram2 accuracy is 0.6883333333333334\n",
    "Stopwords removed and punctuation removed and lowercase:\n",
    "Tfidf ngram2 f1_score is 0.6874877737745918\n",
    "Tfidf ngram2 accuracy is 0.6886666666666666\n",
    "Punctuation removed and lowercase:\n",
    "Tfidf ngram2 f1_score is3 0.6927363814682612\n",
    "Tfidf ngram2 accuracy is3 0.6966666666666667\n",
    "\n",
    "Lower case (22.5K):\n",
    "Tfidf ngram2 f1_score is3 0.6934778512658694\n",
    "Tfidf ngram2 accuracy is3 0.6973333333333334\n",
    "25K:\n",
    "Tfidf ngram2 f1_score is 0.69442532067018\n",
    "Tfidf ngram2 accuracy is 0.6983333333333334\n",
    "27.5K:\n",
    "Tfidf ngram2 f1_score is 0.6922133381924681\n",
    "Tfidf ngram2 accuracy is 0.6963333333333334\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Old preprocessor:\n",
    "20K:\n",
    "Tfidf ngram2 f1_score is 0.7010536400814287\n",
    "Tfidf ngram2 accuracy is 0.7036666666666667\n",
    "22.5K:\n",
    "Tfidf ngram2 f1_score is3 0.7031548610166312\n",
    "Tfidf ngram2 accuracy is3 0.706\n",
    "25K:\n",
    "Tfidf ngram2 f1_score is3 0.7013423559016413\n",
    "Tfidf ngram2 accuracy is3 0.7046666666666667\n",
    "30K:\n",
    "Tfidf ngram2 f1_score is 0.6942318664030661\n",
    "Tfidf ngram2 accuracy is 0.6983333333333334\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bfbdf9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf bigram f1_score is 0.6474050032384886\n",
      "Tfidf bigram accuracy is 0.654\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_tfidf_bigram_model = train_model(naive_bayes.MultinomialNB(alpha=alpha_value), x_train_tfidf_bigram, train_y)\n",
    "y_pred_nb_tfidf_bigram = predict(naive_bayes_tfidf_bigram_model, x_test_tfidf_bigram)\n",
    "nb_tfidf_bigram_f1_score = metrics.f1_score(y_pred_nb_tfidf_bigram, test_y, average='macro')\n",
    "nb_tfidf_bigram_accuracy_score = metrics.accuracy_score(y_pred_nb_tfidf_bigram, test_y)\n",
    "print(\"Tfidf bigram f1_score is \" + str(nb_tfidf_bigram_f1_score))\n",
    "print(\"Tfidf bigram accuracy is \" + str(nb_tfidf_bigram_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8688b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf ngram3 f1_score is 0.7014788671281995\n",
      "Tfidf ngram3 accuracy is 0.7046666666666667\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_tfidf_ngram3_model = train_model(naive_bayes.MultinomialNB(alpha=alpha_value), x_train_tfidf_ngram3, train_y)\n",
    "y_pred_nb_tfidf_ngram3 = predict(naive_bayes_tfidf_ngram3_model, x_test_tfidf_ngram3)\n",
    "nb_tfidf_ngram3_f1_score = metrics.f1_score(y_pred_nb_tfidf_ngram3, test_y, average='macro')\n",
    "nb_tfidf_ngram3_accuracy_score = metrics.accuracy_score(y_pred_nb_tfidf_ngram3, test_y)\n",
    "print(\"Tfidf ngram3 f1_score is \" + str(nb_tfidf_ngram3_f1_score))\n",
    "print(\"Tfidf ngram3 accuracy is \" + str(nb_tfidf_ngram3_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7210024d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTfidf ngram3 f1 and accuracy scores:\\nStopwords and contractions removed:\\n10K\\nTfidf ngram3 f1_score is 0.6599841938542438\\nTfidf ngram3 accuracy is 0.6673333333333333\\n20K\\nTfidf ngram3 f1_score is 0.680573686001227\\nTfidf ngram3 accuracy is 0.6856666666666666\\n30K:\\nTfidf ngram3 f1_score is 0.701722568869707\\nTfidf ngram3 accuracy is 0.7043333333333334\\n32.5K:\\nTfidf ngram3 f1_score is 0.7014788671281995\\nTfidf ngram3 accuracy is 0.7046666666666667\\n35K:\\nTfidf ngram3 f1_score is 0.7006009300510596\\nTfidf ngram3 accuracy is 0.7043333333333334\\n40K:\\nTfidf ngram34 f1_score is 0.6933919572067218\\nTfidf ngram34 accuracy is 0.6986666666666667\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Old preprocessor:\n",
    "Tfidf ngram3 f1 and accuracy scores:\n",
    "Stopwords and contractions removed:\n",
    "10K\n",
    "Tfidf ngram3 f1_score is 0.6599841938542438\n",
    "Tfidf ngram3 accuracy is 0.6673333333333333\n",
    "20K\n",
    "Tfidf ngram3 f1_score is 0.680573686001227\n",
    "Tfidf ngram3 accuracy is 0.6856666666666666\n",
    "30K:\n",
    "Tfidf ngram3 f1_score is 0.701722568869707\n",
    "Tfidf ngram3 accuracy is 0.7043333333333334\n",
    "32.5K:\n",
    "Tfidf ngram3 f1_score is 0.7014788671281995\n",
    "Tfidf ngram3 accuracy is 0.7046666666666667\n",
    "35K:\n",
    "Tfidf ngram3 f1_score is 0.7006009300510596\n",
    "Tfidf ngram3 accuracy is 0.7043333333333334\n",
    "40K:\n",
    "Tfidf ngram34 f1_score is 0.6933919572067218\n",
    "Tfidf ngram34 accuracy is 0.6986666666666667\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec578094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf char f1_score is 0.17134910257328878\n",
      "Tfidf char accuracy is 0.21066666666666667\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_tfidf_char_model = train_model(naive_bayes.MultinomialNB(alpha=alpha_value), x_train_tfidf_char, train_y)\n",
    "y_pred_nb_tfidf_char = predict(naive_bayes_tfidf_char_model, x_test_tfidf_char)\n",
    "nb_tfidf_char_f1_score = metrics.f1_score(y_pred_nb_tfidf_char, test_y, average='macro')\n",
    "nb_tfidf_char_accuracy_score = metrics.accuracy_score(y_pred_nb_tfidf_char, test_y)\n",
    "print(\"Tfidf char f1_score is \" + str(nb_tfidf_char_f1_score))\n",
    "print(\"Tfidf char accuracy is \" + str(nb_tfidf_char_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f848aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tfidf char f1 and accuracy scores:\n",
    "Stopwords and contractions removed:\n",
    "Tfidf char f1_score is 0.17134910257328878\n",
    "Tfidf char accuracy is 0.21066666666666667\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9e721d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Best Result of Feature Engineering:\n",
    "best_x_train = x_train_tfidf_ngram2\n",
    "best_y_train = train_y\n",
    "best_x_test = x_test_tfidf_ngram2\n",
    "best_y_test = test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "254f1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf ngram2 f1_score is 0.6991832045205303\n",
      "Tfidf ngram2 accuracy is 0.7023333333333334\n"
     ]
    }
   ],
   "source": [
    "#Tune hyperparameter alpha\n",
    "alpha_value = 0.45\n",
    "\n",
    "best_model = train_model(naive_bayes.MultinomialNB(alpha=alpha_value), best_x_train, best_y_train)\n",
    "best_y_pred = predict(best_model, best_x_test)\n",
    "best_f1_score = metrics.f1_score(best_y_pred, best_y_test, average='macro')\n",
    "best_accuracy_score = metrics.accuracy_score(best_y_pred, best_y_test)\n",
    "print(\"Tfidf ngram2 f1_score is \" + str(best_f1_score))\n",
    "print(\"Tfidf ngram2 accuracy is \" + str(best_accuracy_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8424ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0.5\\nTfidf ngram2 f1_score is 0.6955135766467679\\nTfidf ngram2 accuracy is 0.699\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "0 (No smoothing - Warning for numberic errors, set to be 1.0e-10):\n",
    "Tfidf ngram2 f1_score is 0.6491445051566886\n",
    "Tfidf ngram2 accuracy is 0.662\n",
    "0.01:\n",
    "Tfidf ngram2 f1_score is 0.6911064705211117\n",
    "Tfidf ngram2 accuracy is 0.6956666666666667\n",
    "0.1:\n",
    "Tfidf ngram2 f1_score is 0.7001699048765011\n",
    "Tfidf ngram2 accuracy is 0.7033333333333334\n",
    "0.2:\n",
    "Tfidf ngram2 f1_score is 0.7046035154671063\n",
    "Tfidf ngram2 accuracy is 0.7073333333333334\n",
    "0.3:\n",
    "Tfidf ngram2 f1_score is 0.7045687893685062\n",
    "Tfidf ngram2 accuracy is 0.707\n",
    "0.4:\n",
    "Tfidf ngram2 f1_score is 0.7056049455059195\n",
    "Tfidf ngram2 accuracy is 0.708\n",
    "0.5:\n",
    "Tfidf ngram2 f1_score is 0.7056318202123468\n",
    "Tfidf ngram2 accuracy is 0.708\n",
    "0.6:\n",
    "Tfidf ngram2 f1_score is 0.7059033059446803\n",
    "Tfidf ngram2 accuracy is 0.7083333333333334\n",
    "0.7:\n",
    "Tfidf ngram2 f1_score is 0.7052258822271097\n",
    "Tfidf ngram2 accuracy is 0.7076666666666667\n",
    "0.8:\n",
    "Tfidf ngram2 f1_score is 0.704470729712227\n",
    "Tfidf ngram2 accuracy is 0.707\n",
    "0.9:\n",
    "Tfidf ngram2 f1_score is 0.7029178732508483\n",
    "Tfidf ngram2 accuracy is 0.7056666666666667\n",
    "1.0:\n",
    "Tfidf ngram2 f1_score is 0.7031548610166312\n",
    "Tfidf ngram2 accuracy is 0.706\n",
    "1.5:\n",
    "Tfidf ngram2 f1_score is 0.6921531157530701\n",
    "Tfidf ngram2 accuracy is 0.6966666666666667\n",
    "2.0:\n",
    "Tfidf ngram2 f1_score is 0.6783961682606644\n",
    "Tfidf ngram2 accuracy is 0.6853333333333333\n",
    "3.0:\n",
    "Tfidf ngram2 f1_score is 0.6498251424597556\n",
    "Tfidf ngram2 accuracy is 0.663\n",
    "'''\n",
    "'''\n",
    "0.3\n",
    "Tfidf ngram2 f1_score is 0.6974093796857893\n",
    "Tfidf ngram2 accuracy is 0.7006666666666667\n",
    "0.4\n",
    "Tfidf ngram2 f1_score is 0.698566213517513\n",
    "Tfidf ngram2 accuracy is 0.7016666666666667\n",
    "0.45\n",
    "Tfidf ngram2 f1_score is 0.6991832045205303\n",
    "Tfidf ngram2 accuracy is 0.7023333333333334\n",
    "0.5\n",
    "Tfidf ngram2 f1_score is 0.6988528321133809\n",
    "Tfidf ngram2 accuracy is 0.702\n",
    "0.6\n",
    "Tfidf ngram2 f1_score is 0.6987096514753162\n",
    "Tfidf ngram2 accuracy is 0.702\n",
    "0.7\n",
    "Tfidf ngram2 f1_score is 0.6978413908116609\n",
    "Tfidf ngram2 accuracy is 0.7013333333333334\n",
    "0.8\n",
    "Tfidf ngram2 f1_score is 0.6960006445411058\n",
    "Tfidf ngram2 accuracy is 0.6996666666666667\n",
    "0.9 \n",
    "Tfidf ngram2 f1_score is 0.6948857836771832\n",
    "Tfidf ngram2 accuracy is 0.6986666666666667\n",
    "1.0\n",
    "Tfidf ngram2 f1_score is 0.69442532067018\n",
    "Tfidf ngram2 accuracy is 0.6983333333333334\n",
    "2.0\n",
    "Tfidf ngram2 f1_score is 0.663420089611259\n",
    "Tfidf ngram2 accuracy is 0.673\n",
    "3.0\n",
    "Tfidf ngram2 f1_score is 0.6361571109560435\n",
    "Tfidf ngram2 accuracy is 0.655\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a019bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.644     0.687     0.665       750\n",
      "           1      0.724     0.507     0.596       750\n",
      "           2      0.633     0.829     0.718       750\n",
      "           3      0.853     0.787     0.818       750\n",
      "\n",
      "    accuracy                          0.702      3000\n",
      "   macro avg      0.713     0.702     0.699      3000\n",
      "weighted avg      0.713     0.702     0.699      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##alpha = 0.45\n",
    "print(classification_report(best_y_test, best_y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83719fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Findings\n",
    "##f1-score relatively high for label 3, means that naive bayes models the reliable news very well.\n",
    "##f1-score for others relatively low as it cant differentiate well between the types of fake news\n",
    "##f1 score for label 2 is a higher than the others since it is the majority label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
